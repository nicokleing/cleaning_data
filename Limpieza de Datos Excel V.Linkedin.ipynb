{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1fe9a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\nicol\\anaconda3\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: et_xmlfile in c:\\users\\nicol\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39f729ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee7b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al archivo Excel que quieres leer\n",
    "archivo_excel = \" \"\n",
    "\n",
    "# Leer todas las hojas del archivo Excel\n",
    "dict_dfs = pd.read_excel(archivo_excel, sheet_name=None)\n",
    "\n",
    "# dict_dfs ahora es un diccionario donde cada clave es el nombre de una hoja, y cada valor es un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894af0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir los nombres de todas las hojas cargadas\n",
    "print(\"Nombres de las hojas cargadas:\", dict_dfs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62525cb4",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "89cb5634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceder al DataFrame de la hoja llamada 'Hoja1'\n",
    "df_hoja1 = dict_dfs['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3a6672a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer una hoja específica del archivo Excel\n",
    "df = pd.read_excel(archivo_excel, sheet_name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bce13306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos los tipos de datos son correctos.\n",
      "No se encontraron valores nulos en columnas críticas.\n",
      "Errores: 'UniqueID' contiene valores duplicados.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def limpiar_caracteres(df):\n",
    "    \"\"\"Limpiar caracteres especiales y corregir tipos mixtos.\"\"\"\n",
    "    for columna in df.columns:\n",
    "        if df[columna].dtype == 'object':\n",
    "            df[columna] = df[columna].str.replace(r\"[^a-zA-Z0-9\\s]+\", '', regex=True).str.strip()\n",
    "        elif any(df[columna].apply(lambda x: isinstance(x, str))):\n",
    "            df[columna] = df[columna].astype(str)\n",
    "\n",
    "def normalizar_fechas(df):\n",
    "    \"\"\"Intentar normalizar columnas que deberían ser de fecha.\"\"\"\n",
    "    for columna in df.columns:\n",
    "        if df[columna].dtype == 'object':\n",
    "            try:\n",
    "                df_temp = pd.to_datetime(df[columna], errors='coerce')\n",
    "                if df_temp.notnull().any():\n",
    "                    df[columna] = df_temp\n",
    "            except Exception as e:\n",
    "                print(f\"No se pudo convertir la columna {columna}: {e}\")\n",
    "\n",
    "def tratar_valores_atipicos(df):\n",
    "    \"\"\"Identificar y tratar valores atípicos en columnas numéricas.\"\"\"\n",
    "    for columna in df.select_dtypes(include=[np.number]).columns:\n",
    "        Q1 = df[columna].quantile(0.25)\n",
    "        Q3 = df[columna].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        filtro = (df[columna] >= (Q1 - 1.5 * IQR)) & (df[columna] <= (Q3 + 1.5 * IQR))\n",
    "        df[columna].where(filtro, df[columna].median(), inplace=True)\n",
    "\n",
    "def limpieza_general(df):\n",
    "    \"\"\"Aplica todas las funciones de limpieza al DataFrame.\"\"\"\n",
    "    limpiar_caracteres(df)\n",
    "    normalizar_fechas(df)\n",
    "    tratar_valores_atipicos(df)\n",
    "\n",
    "def obtener_tipos_esperados(df):\n",
    "    \"\"\"Obtiene los tipos de datos esperados basados en inferencia.\"\"\"\n",
    "    tipos_esperados = df.dtypes.apply(lambda x: x.name).to_dict()\n",
    "    return tipos_esperados\n",
    "\n",
    "def procesar_datos(df):\n",
    "    \"\"\"Procesar y validar los datos.\"\"\"\n",
    "    limpieza_general(df)\n",
    "    tipos_esperados = obtener_tipos_esperados(df)\n",
    "    validar_tipos(df, tipos_esperados)\n",
    "    columnas_criticas = identificar_columnas_criticas(df)\n",
    "    validar_nulos_en_criticas(df, columnas_criticas)\n",
    "    validar_unicidad_uniqueid(df)\n",
    "\n",
    "def validar_tipos(df, tipos_esperados):\n",
    "    \"\"\"Validar tipos de datos esperados.\"\"\"\n",
    "    errores = []\n",
    "    for columna, tipo_esperado in tipos_esperados.items():\n",
    "        if columna in df.columns and df[columna].dtype != tipo_esperado:\n",
    "            errores.append(f\"Tipo incorrecto para {columna}: encontrado {df[columna].dtype}, esperado {tipo_esperado}\")\n",
    "    if errores:\n",
    "        print(\"Errores encontrados en los tipos de datos:\")\n",
    "        for error in errores:\n",
    "            print(error)\n",
    "    else:\n",
    "        print(\"Todos los tipos de datos son correctos.\")\n",
    "\n",
    "def identificar_columnas_criticas(df):\n",
    "    \"\"\"Identificar columnas críticas.\"\"\"\n",
    "    columnas_criticas = []\n",
    "    for columna in df.columns:\n",
    "        # Considerar una columna crítica si no tiene valores nulos\n",
    "        if df[columna].isnull().sum() == 0:\n",
    "            columnas_criticas.append(columna)\n",
    "        # Considerar una columna crítica si tiene una alta cantidad de valores únicos\n",
    "        elif df[columna].nunique() / len(df[columna]) > 0.95:\n",
    "            columnas_criticas.append(columna)\n",
    "    return columnas_criticas\n",
    "\n",
    "def validar_nulos_en_criticas(df, columnas_criticas):\n",
    "    \"\"\"Validar valores nulos en columnas críticas.\"\"\"\n",
    "    errores = []\n",
    "    for columna in columnas_criticas:\n",
    "        if df[columna].isnull().any():\n",
    "            errores.append(f\"Valores nulos encontrados en columna crítica: {columna}\")\n",
    "    if errores:\n",
    "        print(\"Errores encontrados en columnas críticas:\")\n",
    "        for error in errores:\n",
    "            print(error)\n",
    "    else:\n",
    "        print(\"No se encontraron valores nulos en columnas críticas.\")\n",
    "\n",
    "def validar_unicidad_uniqueid(df):\n",
    "    \"\"\"Validar unicidad de UniqueID.\"\"\"\n",
    "    if not df['UniqueID'].is_unique:\n",
    "        print(\"Errores: 'UniqueID' contiene valores duplicados.\")\n",
    "    else:\n",
    "        print(\"'UniqueID' es único.\")\n",
    "\n",
    "\n",
    "archivo_excel = \"\"  \n",
    "df = pd.read_excel(archivo_excel, sheet_name='')\n",
    "procesar_datos(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eed350e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop_duplicates(subset='UniqueID', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b9977b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if df['UniqueID'].is_unique:\n",
    "#    print(\"'UniqueID' es único después del tratamiento.\")\n",
    "#else:\n",
    "#    print(\"Aún existen valores duplicados en 'UniqueID'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a6831da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Ruta donde se guardará el archivo Excel\n",
    "ruta_salida = ''\n",
    "\n",
    "# Nombre del archivo Excel\n",
    "nombre_archivo = ''\n",
    "\n",
    "# Nombre de la hoja en el archivo Excel\n",
    "nombre_hoja = ''\n",
    "\n",
    "# Guardar el DataFrame en un archivo Excel con el nombre de la hoja especificado\n",
    "df.to_excel(ruta_salida + nombre_archivo, sheet_name=nombre_hoja, index=False, engine='openpyxl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3e540a",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "550ac8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer una hoja específica del archivo Excel\n",
    "df = pd.read_excel(archivo_excel, sheet_name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "abcb923a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos los tipos de datos son correctos.\n",
      "No se encontraron valores nulos en columnas críticas.\n",
      "Errores: 'UniqueID' contiene valores duplicados.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def limpiar_caracteres(df):\n",
    "    \"\"\"Limpiar caracteres especiales y corregir tipos mixtos.\"\"\"\n",
    "    for columna in df.columns:\n",
    "        if df[columna].dtype == 'object':\n",
    "            df[columna] = df[columna].str.replace(r\"[^a-zA-Z0-9\\s]+\", '', regex=True).str.strip()\n",
    "        elif any(df[columna].apply(lambda x: isinstance(x, str))):\n",
    "            df[columna] = df[columna].astype(str)\n",
    "\n",
    "def normalizar_fechas(df):\n",
    "    \"\"\"Intentar normalizar columnas que deberían ser de fecha.\"\"\"\n",
    "    for columna in df.columns:\n",
    "        if df[columna].dtype == 'object':\n",
    "            try:\n",
    "                df_temp = pd.to_datetime(df[columna], errors='coerce')\n",
    "                if df_temp.notnull().any():\n",
    "                    df[columna] = df_temp\n",
    "            except Exception as e:\n",
    "                print(f\"No se pudo convertir la columna {columna}: {e}\")\n",
    "\n",
    "def tratar_valores_atipicos(df):\n",
    "    \"\"\"Identificar y tratar valores atípicos en columnas numéricas.\"\"\"\n",
    "    for columna in df.select_dtypes(include=[np.number]).columns:\n",
    "        Q1 = df[columna].quantile(0.25)\n",
    "        Q3 = df[columna].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        filtro = (df[columna] >= (Q1 - 1.5 * IQR)) & (df[columna] <= (Q3 + 1.5 * IQR))\n",
    "        df[columna].where(filtro, df[columna].median(), inplace=True)\n",
    "\n",
    "def limpieza_general(df):\n",
    "    \"\"\"Aplica todas las funciones de limpieza al DataFrame.\"\"\"\n",
    "    limpiar_caracteres(df)\n",
    "    normalizar_fechas(df)\n",
    "    tratar_valores_atipicos(df)\n",
    "\n",
    "def obtener_tipos_esperados(df):\n",
    "    \"\"\"Obtiene los tipos de datos esperados basados en inferencia.\"\"\"\n",
    "    tipos_esperados = df.dtypes.apply(lambda x: x.name).to_dict()\n",
    "    return tipos_esperados\n",
    "\n",
    "def procesar_datos(df):\n",
    "    \"\"\"Procesar y validar los datos.\"\"\"\n",
    "    limpieza_general(df)\n",
    "    tipos_esperados = obtener_tipos_esperados(df)\n",
    "    validar_tipos(df, tipos_esperados)\n",
    "    columnas_criticas = identificar_columnas_criticas(df)\n",
    "    validar_nulos_en_criticas(df, columnas_criticas)\n",
    "    validar_unicidad_uniqueid(df)\n",
    "\n",
    "def validar_tipos(df, tipos_esperados):\n",
    "    \"\"\"Validar tipos de datos esperados.\"\"\"\n",
    "    errores = []\n",
    "    for columna, tipo_esperado in tipos_esperados.items():\n",
    "        if columna in df.columns and df[columna].dtype != tipo_esperado:\n",
    "            errores.append(f\"Tipo incorrecto para {columna}: encontrado {df[columna].dtype}, esperado {tipo_esperado}\")\n",
    "    if errores:\n",
    "        print(\"Errores encontrados en los tipos de datos:\")\n",
    "        for error in errores:\n",
    "            print(error)\n",
    "    else:\n",
    "        print(\"Todos los tipos de datos son correctos.\")\n",
    "\n",
    "def identificar_columnas_criticas(df):\n",
    "    \"\"\"Identificar columnas críticas.\"\"\"\n",
    "    columnas_criticas = []\n",
    "    for columna in df.columns:\n",
    "        # Considerar una columna crítica si no tiene valores nulos\n",
    "        if df[columna].isnull().sum() == 0:\n",
    "            columnas_criticas.append(columna)\n",
    "        # Considerar una columna crítica si tiene una alta cantidad de valores únicos\n",
    "        elif df[columna].nunique() / len(df[columna]) > 0.95:\n",
    "            columnas_criticas.append(columna)\n",
    "    return columnas_criticas\n",
    "\n",
    "def validar_nulos_en_criticas(df, columnas_criticas):\n",
    "    \"\"\"Validar valores nulos en columnas críticas.\"\"\"\n",
    "    errores = []\n",
    "    for columna in columnas_criticas:\n",
    "        if df[columna].isnull().any():\n",
    "            errores.append(f\"Valores nulos encontrados en columna crítica: {columna}\")\n",
    "    if errores:\n",
    "        print(\"Errores encontrados en columnas críticas:\")\n",
    "        for error in errores:\n",
    "            print(error)\n",
    "    else:\n",
    "        print(\"No se encontraron valores nulos en columnas críticas.\")\n",
    "\n",
    "def validar_unicidad_uniqueid(df):\n",
    "    \"\"\"Validar unicidad de UniqueID.\"\"\"\n",
    "    if not df['UniqueID'].is_unique:\n",
    "        print(\"Errores: 'UniqueID' contiene valores duplicados.\")\n",
    "    else:\n",
    "        print(\"'UniqueID' es único.\")\n",
    "\n",
    "\n",
    "archivo_excel = \"\"  \n",
    "df = pd.read_excel(archivo_excel, sheet_name='')\n",
    "procesar_datos(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9eebe967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop_duplicates(subset='UniqueID', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41531ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if df['UniqueID'].is_unique:\n",
    "#    print(\"'UniqueID' es único después del tratamiento.\")\n",
    "#else:\n",
    "#    print(\"Aún existen valores duplicados en 'UniqueID'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6dd34276",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Ruta donde se guardará el archivo Excel\n",
    "ruta_salida = ''\n",
    "\n",
    "# Nombre del archivo Excel\n",
    "nombre_archivo = ''\n",
    "\n",
    "# Nombre de la hoja en el archivo Excel\n",
    "nombre_hoja = ''\n",
    "\n",
    "# Guardar el DataFrame en un archivo Excel con el nombre de la hoja especificado\n",
    "df.to_excel(ruta_salida + nombre_archivo, sheet_name=nombre_hoja, index=False, engine='openpyxl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64641c32",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "7f7abb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer una hoja específica del archivo Excel\n",
    "df = pd.read_excel(archivo_excel, sheet_name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "cca14bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos los tipos de datos son correctos.\n",
      "No se encontraron valores nulos en columnas críticas.\n",
      "Errores: 'UniqueID' contiene valores duplicados.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def limpiar_caracteres(df):\n",
    "    \"\"\"Limpiar caracteres especiales y corregir tipos mixtos.\"\"\"\n",
    "    for columna in df.columns:\n",
    "        if df[columna].dtype == 'object':\n",
    "            df[columna] = df[columna].str.replace(r\"[^a-zA-Z0-9\\s]+\", '', regex=True).str.strip()\n",
    "        elif any(df[columna].apply(lambda x: isinstance(x, str))):\n",
    "            df[columna] = df[columna].astype(str)\n",
    "\n",
    "def normalizar_fechas(df):\n",
    "    \"\"\"Intentar normalizar columnas que deberían ser de fecha.\"\"\"\n",
    "    for columna in df.columns:\n",
    "        if df[columna].dtype == 'object':\n",
    "            try:\n",
    "                df_temp = pd.to_datetime(df[columna], errors='coerce')\n",
    "                if df_temp.notnull().any():\n",
    "                    df[columna] = df_temp\n",
    "            except Exception as e:\n",
    "                print(f\"No se pudo convertir la columna {columna}: {e}\")\n",
    "\n",
    "def tratar_valores_atipicos(df):\n",
    "    \"\"\"Identificar y tratar valores atípicos en columnas numéricas.\"\"\"\n",
    "    for columna in df.select_dtypes(include=[np.number]).columns:\n",
    "        Q1 = df[columna].quantile(0.25)\n",
    "        Q3 = df[columna].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        filtro = (df[columna] >= (Q1 - 1.5 * IQR)) & (df[columna] <= (Q3 + 1.5 * IQR))\n",
    "        df[columna].where(filtro, df[columna].median(), inplace=True)\n",
    "\n",
    "def limpieza_general(df):\n",
    "    \"\"\"Aplica todas las funciones de limpieza al DataFrame.\"\"\"\n",
    "    limpiar_caracteres(df)\n",
    "    normalizar_fechas(df)\n",
    "    tratar_valores_atipicos(df)\n",
    "\n",
    "def obtener_tipos_esperados(df):\n",
    "    \"\"\"Obtiene los tipos de datos esperados basados en inferencia.\"\"\"\n",
    "    tipos_esperados = df.dtypes.apply(lambda x: x.name).to_dict()\n",
    "    return tipos_esperados\n",
    "\n",
    "def procesar_datos(df):\n",
    "    \"\"\"Procesar y validar los datos.\"\"\"\n",
    "    limpieza_general(df)\n",
    "    tipos_esperados = obtener_tipos_esperados(df)\n",
    "    validar_tipos(df, tipos_esperados)\n",
    "    columnas_criticas = identificar_columnas_criticas(df)\n",
    "    validar_nulos_en_criticas(df, columnas_criticas)\n",
    "    validar_unicidad_uniqueid(df)\n",
    "\n",
    "def validar_tipos(df, tipos_esperados):\n",
    "    \"\"\"Validar tipos de datos esperados.\"\"\"\n",
    "    errores = []\n",
    "    for columna, tipo_esperado in tipos_esperados.items():\n",
    "        if columna in df.columns and df[columna].dtype != tipo_esperado:\n",
    "            errores.append(f\"Tipo incorrecto para {columna}: encontrado {df[columna].dtype}, esperado {tipo_esperado}\")\n",
    "    if errores:\n",
    "        print(\"Errores encontrados en los tipos de datos:\")\n",
    "        for error in errores:\n",
    "            print(error)\n",
    "    else:\n",
    "        print(\"Todos los tipos de datos son correctos.\")\n",
    "\n",
    "def identificar_columnas_criticas(df):\n",
    "    \"\"\"Identificar columnas críticas.\"\"\"\n",
    "    columnas_criticas = []\n",
    "    for columna in df.columns:\n",
    "        # Considerar una columna crítica si no tiene valores nulos\n",
    "        if df[columna].isnull().sum() == 0:\n",
    "            columnas_criticas.append(columna)\n",
    "        # Considerar una columna crítica si tiene una alta cantidad de valores únicos\n",
    "        elif df[columna].nunique() / len(df[columna]) > 0.95:\n",
    "            columnas_criticas.append(columna)\n",
    "    return columnas_criticas\n",
    "\n",
    "def validar_nulos_en_criticas(df, columnas_criticas):\n",
    "    \"\"\"Validar valores nulos en columnas críticas.\"\"\"\n",
    "    errores = []\n",
    "    for columna in columnas_criticas:\n",
    "        if df[columna].isnull().any():\n",
    "            errores.append(f\"Valores nulos encontrados en columna crítica: {columna}\")\n",
    "    if errores:\n",
    "        print(\"Errores encontrados en columnas críticas:\")\n",
    "        for error in errores:\n",
    "            print(error)\n",
    "    else:\n",
    "        print(\"No se encontraron valores nulos en columnas críticas.\")\n",
    "\n",
    "def validar_unicidad_uniqueid(df):\n",
    "    \"\"\"Validar unicidad de UniqueID.\"\"\"\n",
    "    if not df['UniqueID'].is_unique:\n",
    "        print(\"Errores: 'UniqueID' contiene valores duplicados.\")\n",
    "    else:\n",
    "        print(\"'UniqueID' es único.\")\n",
    "\n",
    "\n",
    "archivo_excel = \"\"  \n",
    "df = pd.read_excel(archivo_excel, sheet_name='')\n",
    "procesar_datos(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c3a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop_duplicates(subset='UniqueID', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc780b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if df['UniqueID'].is_unique:\n",
    "#    print(\"'UniqueID' es único después del tratamiento.\")\n",
    "#else:\n",
    "#    print(\"Aún existen valores duplicados en 'UniqueID'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c220309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Ruta donde se guardará el archivo Excel\n",
    "ruta_salida = ''\n",
    "\n",
    "# Nombre del archivo Excel\n",
    "nombre_archivo = ''\n",
    "\n",
    "# Nombre de la hoja en el archivo Excel\n",
    "nombre_hoja = ''\n",
    "\n",
    "# Guardar el DataFrame en un archivo Excel con el nombre de la hoja especificado\n",
    "df.to_excel(ruta_salida + nombre_archivo, sheet_name=nombre_hoja, index=False, engine='openpyxl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5802e7",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "fbe84dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer una hoja específica del archivo Excel\n",
    "df = pd.read_excel(archivo_excel, sheet_name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e5cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "53bff1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\anaconda3\\Lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname C identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos los tipos de datos son correctos.\n",
      "No se encontraron valores nulos en columnas críticas.\n",
      "Errores: 'UniqueID' contiene valores duplicados.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def limpiar_caracteres(df):\n",
    "    \"\"\"Limpiar caracteres especiales y corregir tipos mixtos.\"\"\"\n",
    "    for columna in df.columns:\n",
    "        if df[columna].dtype == 'object':\n",
    "            df[columna] = df[columna].str.replace(r\"[^a-zA-Z0-9\\s]+\", '', regex=True).str.strip()\n",
    "        elif any(df[columna].apply(lambda x: isinstance(x, str))):\n",
    "            df[columna] = df[columna].astype(str)\n",
    "\n",
    "def normalizar_fechas(df):\n",
    "    \"\"\"Intentar normalizar columnas que deberían ser de fecha.\"\"\"\n",
    "    for columna in df.columns:\n",
    "        if df[columna].dtype == 'object':\n",
    "            try:\n",
    "                df_temp = pd.to_datetime(df[columna], errors='coerce')\n",
    "                if df_temp.notnull().any():\n",
    "                    df[columna] = df_temp\n",
    "            except Exception as e:\n",
    "                print(f\"No se pudo convertir la columna {columna}: {e}\")\n",
    "\n",
    "def tratar_valores_atipicos(df):\n",
    "    \"\"\"Identificar y tratar valores atípicos en columnas numéricas.\"\"\"\n",
    "    for columna in df.select_dtypes(include=[np.number]).columns:\n",
    "        Q1 = df[columna].quantile(0.25)\n",
    "        Q3 = df[columna].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        filtro = (df[columna] >= (Q1 - 1.5 * IQR)) & (df[columna] <= (Q3 + 1.5 * IQR))\n",
    "        df[columna].where(filtro, df[columna].median(), inplace=True)\n",
    "\n",
    "def limpieza_general(df):\n",
    "    \"\"\"Aplica todas las funciones de limpieza al DataFrame.\"\"\"\n",
    "    limpiar_caracteres(df)\n",
    "    normalizar_fechas(df)\n",
    "    tratar_valores_atipicos(df)\n",
    "\n",
    "def obtener_tipos_esperados(df):\n",
    "    \"\"\"Obtiene los tipos de datos esperados basados en inferencia.\"\"\"\n",
    "    tipos_esperados = df.dtypes.apply(lambda x: x.name).to_dict()\n",
    "    return tipos_esperados\n",
    "\n",
    "def procesar_datos(df):\n",
    "    \"\"\"Procesar y validar los datos.\"\"\"\n",
    "    limpieza_general(df)\n",
    "    tipos_esperados = obtener_tipos_esperados(df)\n",
    "    validar_tipos(df, tipos_esperados)\n",
    "    columnas_criticas = identificar_columnas_criticas(df)\n",
    "    validar_nulos_en_criticas(df, columnas_criticas)\n",
    "    validar_unicidad_uniqueid(df)\n",
    "\n",
    "def validar_tipos(df, tipos_esperados):\n",
    "    \"\"\"Validar tipos de datos esperados.\"\"\"\n",
    "    errores = []\n",
    "    for columna, tipo_esperado in tipos_esperados.items():\n",
    "        if columna in df.columns and df[columna].dtype != tipo_esperado:\n",
    "            errores.append(f\"Tipo incorrecto para {columna}: encontrado {df[columna].dtype}, esperado {tipo_esperado}\")\n",
    "    if errores:\n",
    "        print(\"Errores encontrados en los tipos de datos:\")\n",
    "        for error in errores:\n",
    "            print(error)\n",
    "    else:\n",
    "        print(\"Todos los tipos de datos son correctos.\")\n",
    "\n",
    "def identificar_columnas_criticas(df):\n",
    "    \"\"\"Identificar columnas críticas.\"\"\"\n",
    "    columnas_criticas = []\n",
    "    for columna in df.columns:\n",
    "        # Considerar una columna crítica si no tiene valores nulos\n",
    "        if df[columna].isnull().sum() == 0:\n",
    "            columnas_criticas.append(columna)\n",
    "        # Considerar una columna crítica si tiene una alta cantidad de valores únicos\n",
    "        elif df[columna].nunique() / len(df[columna]) > 0.95:\n",
    "            columnas_criticas.append(columna)\n",
    "    return columnas_criticas\n",
    "\n",
    "def validar_nulos_en_criticas(df, columnas_criticas):\n",
    "    \"\"\"Validar valores nulos en columnas críticas.\"\"\"\n",
    "    errores = []\n",
    "    for columna in columnas_criticas:\n",
    "        if df[columna].isnull().any():\n",
    "            errores.append(f\"Valores nulos encontrados en columna crítica: {columna}\")\n",
    "    if errores:\n",
    "        print(\"Errores encontrados en columnas críticas:\")\n",
    "        for error in errores:\n",
    "            print(error)\n",
    "    else:\n",
    "        print(\"No se encontraron valores nulos en columnas críticas.\")\n",
    "\n",
    "def validar_unicidad_uniqueid(df):\n",
    "    \"\"\"Validar unicidad de UniqueID.\"\"\"\n",
    "    if not df['Unique ID'].is_unique:\n",
    "        print(\"Errores: 'UniqueID' contiene valores duplicados.\")\n",
    "    else:\n",
    "        print(\"'UniqueID' es único.\")\n",
    "\n",
    "\n",
    "archivo_excel = \"\"  \n",
    "df = pd.read_excel(archivo_excel, sheet_name='')\n",
    "procesar_datos(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "5c927d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Order Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1376563",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop_duplicates(subset='UniqueID', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc751371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if df['UniqueID'].is_unique:\n",
    "#    print(\"'UniqueID' es único después del tratamiento.\")\n",
    "#else:\n",
    "#    print(\"Aún existen valores duplicados en 'UniqueID'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "a9b44268",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Ruta donde se guardará el archivo Excel\n",
    "ruta_salida = ''\n",
    "\n",
    "# Nombre del archivo Excel\n",
    "nombre_archivo = ''\n",
    "\n",
    "# Nombre de la hoja en el archivo Excel\n",
    "nombre_hoja = ''\n",
    "\n",
    "# Guardar el DataFrame en un archivo Excel con el nombre de la hoja especificado\n",
    "df.to_excel(ruta_salida + nombre_archivo, sheet_name=nombre_hoja, index=False, engine='openpyxl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6efa728",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1a3827b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer una hoja específica del archivo Excel\n",
    "df = pd.read_excel(archivo_excel, sheet_name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894abfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b5fb1a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos los tipos de datos son correctos.\n",
      "No se encontraron valores nulos en columnas críticas.\n",
      "Errores: 'UniqueID' contiene valores duplicados.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def limpiar_caracteres(df):\n",
    "    \"\"\"Limpiar caracteres especiales y corregir tipos mixtos.\"\"\"\n",
    "    for columna in df.columns:\n",
    "        if pd.api.types.is_string_dtype(df[columna]):  # Verificar si la columna es de tipo string\n",
    "            df[columna] = df[columna].astype(str).str.replace(r\"[^a-zA-Z0-9\\s]+\", '', regex=True).str.strip()\n",
    "    # Convertir las columnas numéricas a enteros y llenar NaNs con 0\n",
    "    df.fillna(0, inplace=True)\n",
    "    df = df.apply(lambda x: pd.to_numeric(x, errors='ignore', downcast='integer'))\n",
    "    # Convertir las columnas de fecha a formato string\n",
    "    for columna in df.select_dtypes(include=['datetime64']).columns:\n",
    "        df[columna] = df[columna].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "\n",
    "def normalizar_fechas(df):\n",
    "    \"\"\"Intentar normalizar columnas que deberían ser de fecha.\"\"\"\n",
    "    for columna in df.columns:\n",
    "        if df[columna].dtype == 'object':\n",
    "            try:\n",
    "                df_temp = pd.to_datetime(df[columna], errors='coerce')\n",
    "                if df_temp.notnull().any():\n",
    "                    df[columna] = df_temp\n",
    "            except Exception as e:\n",
    "                print(f\"No se pudo convertir la columna {columna}: {e}\")\n",
    "\n",
    "def tratar_valores_atipicos(df):\n",
    "    \"\"\"Identificar y tratar valores atípicos en columnas numéricas.\"\"\"\n",
    "    for columna in df.select_dtypes(include=[np.number]).columns:\n",
    "        Q1 = df[columna].quantile(0.25)\n",
    "        Q3 = df[columna].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        filtro = (df[columna] >= (Q1 - 1.5 * IQR)) & (df[columna] <= (Q3 + 1.5 * IQR))\n",
    "        df[columna].where(filtro, df[columna].median(), inplace=True)\n",
    "\n",
    "def limpieza_general(df):\n",
    "    \"\"\"Aplica todas las funciones de limpieza al DataFrame.\"\"\"\n",
    "    limpiar_caracteres(df)\n",
    "    normalizar_fechas(df)\n",
    "    tratar_valores_atipicos(df)\n",
    "\n",
    "def obtener_tipos_esperados(df):\n",
    "    \"\"\"Obtiene los tipos de datos esperados basados en inferencia.\"\"\"\n",
    "    tipos_esperados = df.dtypes.apply(lambda x: x.name).to_dict()\n",
    "    return tipos_esperados\n",
    "\n",
    "def procesar_datos(df):\n",
    "    \"\"\"Procesar y validar los datos.\"\"\"\n",
    "    limpieza_general(df)\n",
    "    tipos_esperados = obtener_tipos_esperados(df)\n",
    "    validar_tipos(df, tipos_esperados)\n",
    "    columnas_criticas = identificar_columnas_criticas(df)\n",
    "    validar_nulos_en_criticas(df, columnas_criticas)\n",
    "    validar_unicidad_uniqueid(df)\n",
    "\n",
    "def validar_tipos(df, tipos_esperados):\n",
    "    \"\"\"Validar tipos de datos esperados.\"\"\"\n",
    "    errores = []\n",
    "    for columna, tipo_esperado in tipos_esperados.items():\n",
    "        if columna in df.columns and df[columna].dtype != tipo_esperado:\n",
    "            errores.append(f\"Tipo incorrecto para {columna}: encontrado {df[columna].dtype}, esperado {tipo_esperado}\")\n",
    "    if errores:\n",
    "        print(\"Errores encontrados en los tipos de datos:\")\n",
    "        for error in errores:\n",
    "            print(error)\n",
    "    else:\n",
    "        print(\"Todos los tipos de datos son correctos.\")\n",
    "\n",
    "def identificar_columnas_criticas(df):\n",
    "    \"\"\"Identificar columnas críticas.\"\"\"\n",
    "    columnas_criticas = []\n",
    "    for columna in df.columns:\n",
    "        # Considerar una columna crítica si no tiene valores nulos\n",
    "        if df[columna].isnull().sum() == 0:\n",
    "            columnas_criticas.append(columna)\n",
    "        # Considerar una columna crítica si tiene una alta cantidad de valores únicos\n",
    "        elif df[columna].nunique() / len(df[columna]) > 0.95:\n",
    "            columnas_criticas.append(columna)\n",
    "    return columnas_criticas\n",
    "\n",
    "def validar_nulos_en_criticas(df, columnas_criticas):\n",
    "    \"\"\"Validar valores nulos en columnas críticas.\"\"\"\n",
    "    errores = []\n",
    "    for columna in columnas_criticas:\n",
    "        if df[columna].isnull().any():\n",
    "            errores.append(f\"Valores nulos encontrados en columna crítica: {columna}\")\n",
    "    if errores:\n",
    "        print(\"Errores encontrados en columnas críticas:\")\n",
    "        for error in errores:\n",
    "            print(error)\n",
    "    else:\n",
    "        print(\"No se encontraron valores nulos en columnas críticas.\")\n",
    "\n",
    "def validar_unicidad_uniqueid(df):\n",
    "    \"\"\"Validar unicidad de UniqueID.\"\"\"\n",
    "    if not df['Unique ID'].is_unique:\n",
    "        print(\"Errores: 'UniqueID' contiene valores duplicados.\")\n",
    "    else:\n",
    "        print(\"'UniqueID' es único.\")\n",
    "\n",
    "\n",
    "archivo_excel = \"\"  \n",
    "df = pd.read_excel(archivo_excel, sheet_name='')\n",
    "procesar_datos(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c26255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop_duplicates(subset='UniqueID', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6175e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if df['UniqueID'].is_unique:\n",
    "#    print(\"'UniqueID' es único después del tratamiento.\")\n",
    "#else:\n",
    "#    print(\"Aún existen valores duplicados en 'UniqueID'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "1e35a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Ruta donde se guardará el archivo Excel\n",
    "ruta_salida = ''\n",
    "\n",
    "# Nombre del archivo Excel\n",
    "nombre_archivo = ''\n",
    "\n",
    "# Nombre de la hoja en el archivo Excel\n",
    "nombre_hoja = ''\n",
    "\n",
    "# Guardar el DataFrame en un archivo Excel con el nombre de la hoja especificado\n",
    "df.to_excel(ruta_salida + nombre_archivo, sheet_name=nombre_hoja, index=False, engine='openpyxl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293744ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
